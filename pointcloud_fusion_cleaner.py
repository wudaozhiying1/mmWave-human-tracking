#!/usr/bin/env python3
"""
åŸºäºTIé›·è¾¾ç‚¹äº‘çš„å¤šå¸§èåˆä¸èšç±»æ¸…æ´—å™¨
å‚è€ƒè®ºæ–‡ï¼šåŸºäºæ¯«ç±³æ³¢é›·è¾¾ä¸‰ç»´ç‚¹äº‘çš„äººä½“åŠ¨ä½œè¯†åˆ«æ•°æ®é›†ä¸æ–¹æ³•
"""

import json
import os
import numpy as np
from datetime import datetime
import glob
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

class PointCloudFusionCleaner:
    def __init__(self, data_folder="radar_data"):
        """
        åˆå§‹åŒ–ç‚¹äº‘èåˆæ¸…æ´—å™¨
        Args:
            data_folder: æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
        """
        self.data_folder = data_folder
        self.frames_per_sample = 6  # 6å¸§èåˆä¸ºä¸€ä¸ªæ ·æœ¬
        
    def load_pointcloud_data(self, file_path):
        """åŠ è½½ç‚¹äº‘æ•°æ®æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return data
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return []
    
    def extract_xyz_points(self, point_data):
        """
        ä»ç‚¹äº‘æ•°æ®ä¸­æå–x,y,zåæ ‡
        Args:
            point_data: ç‚¹äº‘æ•°æ®åˆ—è¡¨
        Returns:
            points: numpyæ•°ç»„ï¼Œå½¢çŠ¶ä¸º(n_points, 3)
        """
        points = []
        for item in point_data:
            if 'x' in item and 'y' in item and 'z' in item:
                points.append([item['x'], item['y'], item['z']])
        
        return np.array(points) if points else np.empty((0, 3))
    
    def group_frames_by_time(self, point_data, time_window=0.1):
        """
        æŒ‰æ—¶é—´çª—å£åˆ†ç»„å¸§æ•°æ®ï¼ˆæ›´çŸ­çš„æ—¶é—´çª—å£ä»¥ä¿æŒå¸§çš„è¿ç»­æ€§ï¼‰
        Args:
            point_data: ç‚¹äº‘æ•°æ®åˆ—è¡¨
            time_window: æ—¶é—´çª—å£ï¼ˆç§’ï¼‰
        Returns:
            grouped_frames: åˆ†ç»„åçš„å¸§æ•°æ®
        """
        if not point_data:
            return []
        
        # æŒ‰æ—¶é—´æˆ³æ’åº
        sorted_data = sorted(point_data, key=lambda x: x.get('timestamp', 0))
        
        grouped_frames = []
        current_group = []
        last_timestamp = None
        
        for item in sorted_data:
            timestamp = item.get('timestamp', 0)
            
            if last_timestamp is None or (timestamp - last_timestamp) <= time_window:
                current_group.append(item)
            else:
                if current_group:
                    grouped_frames.append(current_group)
                current_group = [item]
            
            last_timestamp = timestamp
        
        # æ·»åŠ æœ€åä¸€ç»„
        if current_group:
            grouped_frames.append(current_group)
        
        return grouped_frames
    
    def multi_frame_fusion(self, frame_groups):
        """
        å¤šå¸§èåˆï¼šå°†6å¸§çš„ç‚¹äº‘æ•°æ®èåˆ
        Args:
            frame_groups: åˆ†ç»„åçš„å¸§æ•°æ®
        Returns:
            fused_points: èåˆåçš„ç‚¹äº‘æ•°æ®
        """
        all_points = []
        
        for frame_group in frame_groups:
            points = self.extract_xyz_points(frame_group)
            if len(points) > 0:
                all_points.append(points)
        
        if not all_points:
            return np.empty((0, 3))
        
        # å°†æ‰€æœ‰å¸§çš„ç‚¹äº‘åˆå¹¶
        fused_points = np.vstack(all_points)
        return fused_points
    
    def dbscan_clustering(self, points, eps=0.5, min_samples=3):
        """
        DBSCANèšç±»ï¼Œåˆ†ç¦»ç›®æ ‡å’Œå™ªå£°ç‚¹
        Args:
            points: ç‚¹äº‘æ•°æ®ï¼Œå½¢çŠ¶ä¸º(n_points, 3)
            eps: é‚»åŸŸåŠå¾„
            min_samples: æœ€å°æ ·æœ¬æ•°
        Returns:
            target_points: ç›®æ ‡ç‚¹äº‘
            noise_points: å™ªå£°ç‚¹äº‘
        """
        if len(points) == 0:
            return np.empty((0, 3)), np.empty((0, 3))
        
        # æ ‡å‡†åŒ–æ•°æ®
        scaler = StandardScaler()
        points_scaled = scaler.fit_transform(points)
        
        # DBSCANèšç±»
        clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(points_scaled)
        labels = clustering.labels_
        
        # åˆ†ç¦»ç›®æ ‡å’Œå™ªå£°ç‚¹
        target_mask = labels != -1  # éå™ªå£°ç‚¹
        noise_mask = labels == -1   # å™ªå£°ç‚¹
        
        target_points = points[target_mask]
        noise_points = points[noise_mask]
        
        return target_points, noise_points
    
    def calculate_density_features(self, points):
        """
        è®¡ç®—ç‚¹äº‘å¯†åº¦ç‰¹å¾
        Args:
            points: ç‚¹äº‘æ•°æ®
        Returns:
            features: å¯†åº¦ç‰¹å¾å­—å…¸
        """
        if len(points) == 0:
            return {
                'num_points': 0,
                'point_density': 0.0,
                'spatial_volume': 0.0,
                'concentration_ratio': 0.0
            }
        
        # åŸºæœ¬ç»Ÿè®¡
        num_points = len(points)
        
        # è®¡ç®—ç©ºé—´èŒƒå›´
        x_range = np.max(points[:, 0]) - np.min(points[:, 0])
        y_range = np.max(points[:, 1]) - np.min(points[:, 1])
        z_range = np.max(points[:, 2]) - np.min(points[:, 2])
        
        # ç©ºé—´ä½“ç§¯
        spatial_volume = x_range * y_range * z_range
        
        # ç‚¹äº‘å¯†åº¦ï¼ˆç‚¹æ•°/ä½“ç§¯ï¼‰
        point_density = num_points / (spatial_volume + 1e-6)
        
        # è®¡ç®—ç‚¹äº‘ä¸­å¿ƒ
        center = np.mean(points, axis=0)
        
        # è®¡ç®—åˆ°ä¸­å¿ƒçš„è·ç¦»
        distances = np.linalg.norm(points - center, axis=1)
        
        # é›†ä¸­åº¦æ¯”ç‡ï¼ˆè¿‘è·ç¦»ç‚¹çš„æ¯”ä¾‹ï¼‰
        concentration_ratio = np.sum(distances < np.mean(distances)) / num_points
        
        return {
            'num_points': num_points,
            'point_density': float(point_density),
            'spatial_volume': float(spatial_volume),
            'concentration_ratio': float(concentration_ratio),
            'x_range': float(x_range),
            'y_range': float(y_range),
            'z_range': float(z_range),
            'center_x': float(center[0]),
            'center_y': float(center[1]),
            'center_z': float(center[2])
        }
    
    def create_fused_sample(self, frame_groups):
        """
        åˆ›å»ºèåˆæ ·æœ¬
        Args:
            frame_groups: åˆ†ç»„åçš„å¸§æ•°æ®
        Returns:
            sample: èåˆåçš„æ ·æœ¬
        """
        if len(frame_groups) < self.frames_per_sample:
            return None
        
        # å–å‰6å¸§è¿›è¡Œèåˆ
        sample_frames = frame_groups[:self.frames_per_sample]
        
        # å¤šå¸§èåˆ
        fused_points = self.multi_frame_fusion(sample_frames)
        
        if len(fused_points) == 0:
            return None
        
        # DBSCANèšç±»
        target_points, noise_points = self.dbscan_clustering(fused_points)
        
        # è®¡ç®—ç‰¹å¾
        target_features = self.calculate_density_features(target_points)
        noise_features = self.calculate_density_features(noise_points)
        
        # åˆ›å»ºæ ·æœ¬
        sample = {
            'sample_id': 0,  # å°†åœ¨å¤–éƒ¨è®¾ç½®
            'start_timestamp': sample_frames[0][0].get('timestamp', 0),
            'end_timestamp': sample_frames[-1][-1].get('timestamp', 0),
            'num_frames': len(sample_frames),
            'target_points': target_points.tolist() if len(target_points) > 0 else [],
            'noise_points': noise_points.tolist() if len(noise_points) > 0 else [],
            'target_features': target_features,
            'noise_features': noise_features,
            'fusion_ratio': len(target_points) / (len(target_points) + len(noise_points) + 1e-6)
        }
        
        return sample
    
    def visualize_fusion_result(self, original_points, fused_points, target_points, noise_points, save_path=None):
        """
        å¯è§†åŒ–èåˆç»“æœï¼ˆç±»ä¼¼è®ºæ–‡Figure 10ï¼‰
        Args:
            original_points: åŸå§‹å•å¸§ç‚¹äº‘
            fused_points: èåˆåçš„ç‚¹äº‘
            target_points: ç›®æ ‡ç‚¹äº‘
            noise_points: å™ªå£°ç‚¹äº‘
            save_path: ä¿å­˜è·¯å¾„
        """
        fig = plt.figure(figsize=(15, 6))
        
        # åŸå§‹å•å¸§ç‚¹äº‘
        ax1 = fig.add_subplot(121, projection='3d')
        if len(original_points) > 0:
            ax1.scatter(original_points[:, 0], original_points[:, 1], original_points[:, 2], 
                       c='blue', marker='o', alpha=0.6, label='Original Points')
        ax1.set_xlabel('X')
        ax1.set_ylabel('Y')
        ax1.set_zlabel('Z')
        ax1.set_title('(a) å•å¸§ç‚¹äº‘ (Single Frame Point Cloud)')
        ax1.legend()
        
        # èåˆèšç±»åçš„ç‚¹äº‘
        ax2 = fig.add_subplot(122, projection='3d')
        if len(target_points) > 0:
            ax2.scatter(target_points[:, 0], target_points[:, 1], target_points[:, 2], 
                       c='red', marker='o', alpha=0.8, label='Target')
        if len(noise_points) > 0:
            ax2.scatter(noise_points[:, 0], noise_points[:, 1], noise_points[:, 2], 
                       c='black', marker='x', alpha=0.6, label='Noise')
        ax2.set_xlabel('X')
        ax2.set_ylabel('Y')
        ax2.set_zlabel('Z')
        ax2.set_title('(b) èåˆèšç±»åçš„ç‚¹äº‘ (Fused and Clustered Point Cloud)')
        ax2.legend()
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š å¯è§†åŒ–ç»“æœå·²ä¿å­˜: {save_path}")
        
        plt.show()
    
    def clean_and_save_data(self, input_file, output_file, visualize=False):
        """
        æ¸…æ´—å¹¶ä¿å­˜æ•°æ®
        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            visualize: æ˜¯å¦ç”Ÿæˆå¯è§†åŒ–
        """
        print(f"ğŸ§¹ æ¸…æ´—æ•°æ®: {input_file}")
        
        # åŠ è½½åŸå§‹æ•°æ®
        raw_data = self.load_pointcloud_data(input_file)
        if not raw_data:
            print(f"âš ï¸  æ–‡ä»¶ä¸ºç©ºæˆ–åŠ è½½å¤±è´¥: {input_file}")
            return
        
        print(f"ğŸ“Š åŸå§‹æ•°æ®: {len(raw_data)} ä¸ªæ•°æ®ç‚¹")
        
        # æŒ‰æ—¶é—´åˆ†ç»„
        frame_groups = self.group_frames_by_time(raw_data)
        print(f"ğŸ“¦ åˆ†ç»„å: {len(frame_groups)} ä¸ªæ—¶é—´çª—å£")
        
        # åˆ›å»ºèåˆæ ·æœ¬
        samples = []
        for i in range(0, len(frame_groups), self.frames_per_sample):
            sample_frames = frame_groups[i:i + self.frames_per_sample]
            
            if len(sample_frames) == self.frames_per_sample:
                # è·å–åŸå§‹å•å¸§ç‚¹äº‘ç”¨äºå¯¹æ¯”
                original_points = self.extract_xyz_points(sample_frames[0])
                
                # åˆ›å»ºèåˆæ ·æœ¬
                sample = self.create_fused_sample(sample_frames)
                if sample:
                    sample['sample_id'] = len(samples)
                    samples.append(sample)
                    
                    # å¯è§†åŒ–ç¬¬ä¸€ä¸ªæ ·æœ¬
                    if visualize and len(samples) == 1:
                        fused_points = np.array(sample['target_points'] + sample['noise_points'])
                        target_points = np.array(sample['target_points'])
                        noise_points = np.array(sample['noise_points'])
                        
                        viz_path = output_file.replace('.json', '_visualization.png')
                        self.visualize_fusion_result(original_points, fused_points, target_points, noise_points, viz_path)
        
        print(f"ğŸ¯ åˆ›å»ºæ ·æœ¬: {len(samples)} ä¸ªèåˆæ ·æœ¬")
        
        # ä¿å­˜æ¸…æ´—åçš„æ•°æ®
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(samples, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ æ¸…æ´—åæ•°æ®å·²ä¿å­˜: {output_file}")
        
        return samples
    
    def process_all_data(self, visualize=False):
        """å¤„ç†æ‰€æœ‰æ•°æ®æ–‡ä»¶å¤¹"""
        print("ğŸš€ å¼€å§‹åŸºäºè®ºæ–‡æ–¹æ³•æ¸…æ´—æ‰€æœ‰é›·è¾¾æ•°æ®")
        print("=" * 60)
        
        # æŸ¥æ‰¾æ‰€æœ‰æ•°æ®æ–‡ä»¶å¤¹
        data_dirs = ['sit', 'squat', 'stand']
        
        for data_dir in data_dirs:
            dir_path = os.path.join(self.data_folder, data_dir)
            if not os.path.exists(dir_path):
                print(f"âš ï¸  æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {dir_path}")
                continue
            
            print(f"\nğŸ“ å¤„ç†æ–‡ä»¶å¤¹: {data_dir}")
            
            # æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
            json_files = glob.glob(os.path.join(dir_path, "**/*.json"), recursive=True)
            
            if not json_files:
                print(f"âš ï¸  æœªæ‰¾åˆ°JSONæ–‡ä»¶: {dir_path}")
                continue
            
            # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
            output_dir = os.path.join(self.data_folder, f"{data_dir}_fused")
            os.makedirs(output_dir, exist_ok=True)
            
            total_samples = 0
            
            for json_file in json_files:
                # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
                rel_path = os.path.relpath(json_file, dir_path)
                output_file = os.path.join(output_dir, f"fused_{rel_path}")
                
                # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(output_file), exist_ok=True)
                
                # æ¸…æ´—æ•°æ®
                samples = self.clean_and_save_data(json_file, output_file, visualize)
                if samples:
                    total_samples += len(samples)
            
            print(f"âœ… {data_dir} å¤„ç†å®Œæˆï¼Œå…±ç”Ÿæˆ {total_samples} ä¸ªèåˆæ ·æœ¬")
        
        print("\nğŸ‰ æ‰€æœ‰æ•°æ®èåˆæ¸…æ´—å®Œæˆï¼")

    def process_stand_data(self, visualize=False):
        """ä¸“é—¨å¤„ç†standæ•°æ®"""
        print("ğŸš€ å¼€å§‹å¤„ç†standç‚¹äº‘æ•°æ®")
        print("=" * 60)
        
        # standåŸå§‹æ•°æ®ç›®å½•
        stand_dir = os.path.join(self.data_folder, 'stand', 'pointcloud')
        if not os.path.exists(stand_dir):
            print(f"âš ï¸  standç›®å½•ä¸å­˜åœ¨: {stand_dir}")
            return
        
        # è¾“å‡ºç›®å½•
        output_dir = os.path.join(self.data_folder, 'stand_data', 'fused_pointcloud')
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"ğŸ“ è¾“å…¥ç›®å½•: {stand_dir}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        
        # æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
        json_files = glob.glob(os.path.join(stand_dir, "*.json"))
        
        if not json_files:
            print(f"âš ï¸  æœªæ‰¾åˆ°JSONæ–‡ä»¶: {stand_dir}")
            return
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(json_files)} ä¸ªç‚¹äº‘æ–‡ä»¶")
        
        total_samples = 0
        
        for json_file in json_files:
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            base_name = os.path.basename(json_file)
            output_file = os.path.join(output_dir, base_name)
            
            print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {base_name}")
            
            # æ¸…æ´—æ•°æ®
            samples = self.clean_and_save_data(json_file, output_file, visualize)
            if samples:
                total_samples += len(samples)
                print(f"âœ… ç”Ÿæˆ {len(samples)} ä¸ªèåˆæ ·æœ¬")
        
        print(f"\nğŸ‰ standæ•°æ®å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“Š æ€»å…±ç”Ÿæˆ {total_samples} ä¸ªèåˆæ ·æœ¬")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        
        return total_samples

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§¹ åŸºäºè®ºæ–‡æ–¹æ³•çš„ç‚¹äº‘èåˆæ¸…æ´—å™¨")
    print("=" * 60)
    
    # åˆ›å»ºæ¸…æ´—å™¨
    cleaner = PointCloudFusionCleaner("radar_data")
    
    # ä¸“é—¨å¤„ç†standæ•°æ®
    cleaner.process_stand_data(visualize=True)

if __name__ == "__main__":
    main() 