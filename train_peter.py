#!/usr/bin/env python3
"""
åŸºäºè®ºæ–‡æ–¹æ³•çš„PETeræ¨¡å‹è®­ç»ƒè„šæœ¬
ä½¿ç”¨sit_data, squat_data, stand_dataä¸‰ç»„æ•°æ®è¿›è¡Œè®­ç»ƒ
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import json
import os
import glob
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
from peter_network import PETerNetwork, DataProcessor
import warnings
warnings.filterwarnings('ignore')

class RadarPointCloudDataset(Dataset):
    """é›·è¾¾ç‚¹äº‘æ•°æ®é›†ç±»"""
    
    def __init__(self, data_paths, labels, sample_indices, num_points=100, num_frames=25, transform=None, augment=False):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        Args:
            data_paths: æ•°æ®æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            labels: å¯¹åº”çš„æ ‡ç­¾åˆ—è¡¨
            sample_indices: æ ·æœ¬ç´¢å¼•åˆ—è¡¨
            num_points: æ¯å¸§é‡‡æ ·ç‚¹æ•°
            num_frames: æ¯ä¸ªæ ·æœ¬çš„å¸§æ•°
            transform: æ•°æ®å˜æ¢
            augment: æ˜¯å¦è¿›è¡Œæ•°æ®å¢å¼º
        """
        self.data_paths = data_paths
        self.labels = labels
        self.sample_indices = sample_indices
        self.num_points = num_points
        self.num_frames = num_frames
        self.transform = transform
        self.augment = augment
        
        # æ ‡ç­¾æ˜ å°„
        self.label_to_idx = {'sit': 0, 'squat': 1, 'stand': 2}
        self.idx_to_label = {0: 'sit', 1: 'squat', 2: 'stand'}
        
    def __len__(self):
        return len(self.data_paths)
    
    def __getitem__(self, idx):
        # åŠ è½½æ•°æ®
        with open(self.data_paths[idx], 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # è·å–æŒ‡å®šç´¢å¼•çš„æ ·æœ¬
        sample_idx = self.sample_indices[idx]
        sample = data[sample_idx]
        
        # è·å–ç›®æ ‡ç‚¹äº‘æ•°æ®
        target_points = sample['target_points']
        if len(target_points) == 0:
            # å¦‚æœæ²¡æœ‰ç›®æ ‡ç‚¹ï¼Œåˆ›å»ºé›¶å¡«å……æ•°æ®
            target_points = np.zeros((self.num_points, 3))
        else:
            target_points = np.array(target_points)
        
        # æ ‡å‡†åŒ–ç‚¹äº‘
        target_points = self.normalize_point_cloud(target_points)
        
        # é‡‡æ ·å›ºå®šæ•°é‡çš„ç‚¹
        target_points = self.sample_points(target_points, self.num_points)
        
        # æ•°æ®å¢å¼ºï¼ˆä»…å¯¹è®­ç»ƒé›†ï¼‰
        if self.augment:
            target_points = self.augment_point_cloud(target_points)
        
        # æ·»åŠ å¼ºåº¦ä¿¡æ¯ï¼ˆå¦‚æœæ²¡æœ‰åˆ™è®¾ä¸º1ï¼‰
        intensity = np.ones((len(target_points), 1))
        target_points = np.hstack([target_points, intensity])
        
        # åˆ›å»ºæ—¶åºåºåˆ—ï¼ˆè¿™é‡Œç®€åŒ–ä¸ºå•å¸§ï¼Œå®é™…åº”è¯¥æœ‰å¤šå¸§ï¼‰
        # ä¸ºäº†é€‚é…æ¨¡å‹ï¼Œæˆ‘ä»¬é‡å¤å•å¸§æ•°æ®
        sequence = np.tile(target_points, (self.num_frames, 1, 1))
        
        # è½¬æ¢ä¸ºå¼ é‡
        sequence = torch.FloatTensor(sequence)
        label = torch.LongTensor([self.label_to_idx[self.labels[idx]]])
        
        if self.transform:
            sequence = self.transform(sequence)
        
        return sequence, label
    
    def normalize_point_cloud(self, points):
        """æ ‡å‡†åŒ–ç‚¹äº‘"""
        if len(points) == 0:
            return points
        
        # ä¸­å¿ƒåŒ–
        centroid = np.mean(points[:, :3], axis=0)
        points[:, :3] = points[:, :3] - centroid
        
        # ç¼©æ”¾åˆ°å•ä½çƒ
        max_dist = np.max(np.linalg.norm(points[:, :3], axis=1))
        if max_dist > 0:
            points[:, :3] = points[:, :3] / max_dist
        
        return points
    
    def sample_points(self, points, num_points):
        """å›ºå®šé‡‡æ ·ç‚¹æ•°"""
        if len(points) == 0:
            return np.zeros((num_points, 3))
        
        if len(points) >= num_points:
            # éšæœºé‡‡æ ·
            indices = np.random.choice(len(points), num_points, replace=False)
            return points[indices]
        else:
            # é‡å¤é‡‡æ ·
            indices = np.random.choice(len(points), num_points, replace=True)
            return points[indices]
    
    def augment_point_cloud(self, points):
        """ç‚¹äº‘æ•°æ®å¢å¼º"""
        if len(points) == 0:
            return points
        
        # éšæœºæ—‹è½¬
        if np.random.random() < 0.5:
            angle = np.random.uniform(-0.1, 0.1)  # å°è§’åº¦æ—‹è½¬
            cos_a, sin_a = np.cos(angle), np.sin(angle)
            rotation_matrix = np.array([[cos_a, -sin_a, 0], [sin_a, cos_a, 0], [0, 0, 1]])
            points[:, :3] = points[:, :3] @ rotation_matrix.T
        
        # éšæœºå™ªå£°
        if np.random.random() < 0.3:
            noise = np.random.normal(0, 0.01, points[:, :3].shape)
            points[:, :3] += noise
        
        # éšæœºç¼©æ”¾
        if np.random.random() < 0.3:
            scale = np.random.uniform(0.95, 1.05)
            points[:, :3] *= scale
        
        return points

def load_training_data():
    """åŠ è½½è®­ç»ƒæ•°æ®"""
    print("ğŸ“Š åŠ è½½è®­ç»ƒæ•°æ®...")
    
    data_paths = []
    labels = []
    sample_indices = []
    
    # æ•°æ®ç›®å½•é…ç½®
    data_config = {
        'sit': 'radar_data/sit_improved_fused/improved_fused_pointcloud',
        'squat': 'radar_data/squat_improved_fused/improved_fused_pointcloud', 
        'stand': 'radar_data/stand_improved_fused/improved_fused_pointcloud'
    }
    
    for action, data_dir in data_config.items():
        if os.path.exists(data_dir):
            json_files = glob.glob(os.path.join(data_dir, "*.json"))
            print(f"   ğŸ“ {action}: {len(json_files)} ä¸ªæ–‡ä»¶")
            
            for json_file in json_files:
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    if isinstance(data, list):
                        # æ¯ä¸ªæ ·æœ¬ä½œä¸ºä¸€ä¸ªæ•°æ®ç‚¹
                        for i, sample in enumerate(data):
                            if isinstance(sample, dict) and 'target_points' in sample:
                                data_paths.append(json_file)
                                labels.append(action)
                                sample_indices.append(i)
                except Exception as e:
                    print(f"   âŒ è¯»å–å¤±è´¥ {json_file}: {e}")
    
    print(f"ğŸ“Š æ€»æ•°æ®ç‚¹: {len(data_paths)}")
    
    # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
    label_counts = {}
    for label in labels:
        label_counts[label] = label_counts.get(label, 0) + 1
    print(f"ğŸ“Š æ ‡ç­¾åˆ†å¸ƒ: {label_counts}")
    
    return data_paths, labels, sample_indices

def create_data_loaders(data_paths, labels, sample_indices, batch_size=32, test_size=0.2, val_size=0.1):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    print("ğŸ”§ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    
    # åˆ’åˆ†è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†
    X_temp, X_test, y_temp, y_test, idx_temp, idx_test = train_test_split(
        data_paths, labels, sample_indices, test_size=test_size, stratify=labels, random_state=42
    )
    
    X_train, X_val, y_train, y_val, idx_train, idx_val = train_test_split(
        X_temp, y_temp, idx_temp, test_size=val_size, stratify=y_temp, random_state=42
    )
    
    print(f"ğŸ“Š è®­ç»ƒé›†: {len(X_train)} ä¸ªæ ·æœ¬")
    print(f"ğŸ“Š éªŒè¯é›†: {len(X_val)} ä¸ªæ ·æœ¬") 
    print(f"ğŸ“Š æµ‹è¯•é›†: {len(X_test)} ä¸ªæ ·æœ¬")
    
    # åˆ›å»ºæ•°æ®é›†ï¼ˆè®­ç»ƒé›†ä½¿ç”¨æ•°æ®å¢å¼ºï¼‰
    train_dataset = RadarPointCloudDataset(X_train, y_train, idx_train, augment=True)
    val_dataset = RadarPointCloudDataset(X_val, y_val, idx_val, augment=False)
    test_dataset = RadarPointCloudDataset(X_test, y_test, idx_test, augment=False)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)
    
    return train_loader, val_loader, test_loader, y_train

def train_model(model, train_loader, val_loader, num_epochs=50, learning_rate=1e-4, device='cpu', class_weights=None):
    """è®­ç»ƒæ¨¡å‹"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    
    # ç§»åŠ¨åˆ°è®¾å¤‡
    model = model.to(device)
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    if class_weights is not None:
        class_weights = torch.FloatTensor(class_weights).to(device)
        criterion = nn.CrossEntropyLoss(weight=class_weights)
        print(f"ğŸ“Š ä½¿ç”¨åŠ æƒæŸå¤±å‡½æ•°ï¼Œæƒé‡: {class_weights.cpu().numpy()}")
    else:
        criterion = nn.CrossEntropyLoss()
        print("ğŸ“Š ä½¿ç”¨æ ‡å‡†æŸå¤±å‡½æ•°")
    
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)
    
    # è®­ç»ƒå†å²
    train_losses = []
    val_losses = []
    train_accs = []
    val_accs = []
    
    best_val_acc = 0.0
    patience = 10
    patience_counter = 0
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.squeeze().to(device)
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = output.max(1)
            train_total += target.size(0)
            train_correct += predicted.eq(target).sum().item()
            
            if batch_idx % 10 == 0:
                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, '
                      f'Loss: {loss.item():.4f}')
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(device), target.squeeze().to(device)
                output = model(data)
                loss = criterion(output, target)
                
                val_loss += loss.item()
                _, predicted = output.max(1)
                val_total += target.size(0)
                val_correct += predicted.eq(target).sum().item()
        
        # è®¡ç®—å‡†ç¡®ç‡
        train_acc = 100. * train_correct / train_total
        val_acc = 100. * val_correct / val_total
        
        train_losses.append(train_loss / len(train_loader))
        val_losses.append(val_loss / len(val_loader))
        train_accs.append(train_acc)
        val_accs.append(val_acc)
        
        print(f'Epoch {epoch+1}/{num_epochs}:')
        print(f'  Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_acc:.2f}%')
        print(f'  Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_acc:.2f}%')
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()
        
        # æ—©åœ
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            torch.save(model.state_dict(), 'best_peter_model.pth')
            print(f'  ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (Val Acc: {val_acc:.2f}%)')
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f'  â¹ï¸  æ—©åœè®­ç»ƒ (Patience: {patience})')
                break
    
    return train_losses, val_losses, train_accs, val_accs

def evaluate_model(model, test_loader, device='cpu'):
    """è¯„ä¼°æ¨¡å‹"""
    print("ğŸ“Š è¯„ä¼°æ¨¡å‹...")
    
    model.eval()
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.squeeze().to(device)
            output = model(data)
            _, predicted = output.max(1)
            
            all_predictions.extend(predicted.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
    
    # è®¡ç®—å‡†ç¡®ç‡
    accuracy = 100. * sum(p == t for p, t in zip(all_predictions, all_targets)) / len(all_targets)
    print(f'ğŸ“Š æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.2f}%')
    
    # åˆ†ç±»æŠ¥å‘Š
    print('\nğŸ“‹ åˆ†ç±»æŠ¥å‘Š:')
    print(classification_report(all_targets, all_predictions, 
                               target_names=['sit', 'squat', 'stand']))
    
    return all_predictions, all_targets, accuracy

def plot_training_history(train_losses, val_losses, train_accs, val_accs):
    """ç»˜åˆ¶è®­ç»ƒå†å²"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # æŸå¤±æ›²çº¿
    ax1.plot(train_losses, label='Train Loss')
    ax1.plot(val_losses, label='Val Loss')
    ax1.set_title('Training and Validation Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True)
    
    # å‡†ç¡®ç‡æ›²çº¿
    ax2.plot(train_accs, label='Train Acc')
    ax2.plot(val_accs, label='Val Acc')
    ax2.set_title('Training and Validation Accuracy')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy (%)')
    ax2.legend()
    ax2.grid(True)
    
    plt.tight_layout()
    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')
    plt.show()

def calculate_class_weights(labels):
    """è®¡ç®—ç±»åˆ«æƒé‡ä»¥å¤„ç†æ•°æ®ä¸å¹³è¡¡"""
    from collections import Counter
    
    # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°
    class_counts = Counter(labels)
    total_samples = len(labels)
    
    # è®¡ç®—æƒé‡ï¼ˆæ ·æœ¬æ•°è¶Šå°‘ï¼Œæƒé‡è¶Šå¤§ï¼‰
    weights = []
    for i in range(3):  # 3ä¸ªç±»åˆ«ï¼šsit, squat, stand
        class_name = ['sit', 'squat', 'stand'][i]
        if class_name in class_counts:
            # ä½¿ç”¨é€†é¢‘ç‡æƒé‡
            weight = total_samples / (len(class_counts) * class_counts[class_name])
            weights.append(weight)
        else:
            weights.append(1.0)
    
    print(f"ğŸ“Š ç±»åˆ«æƒé‡: {weights}")
    return weights

def plot_confusion_matrix(y_true, y_pred):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['sit', 'squat', 'stand'],
                yticklabels=['sit', 'squat', 'stand'])
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.tight_layout()
    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.show()

def print_training_summary(class_weights, accuracy):
    """æ‰“å°è®­ç»ƒæ€»ç»“"""
    print("\n" + "="*60)
    print("ğŸ¯ è®­ç»ƒæ€»ç»“")
    print("="*60)
    print(f"ğŸ“Š ç±»åˆ«æƒé‡: {class_weights}")
    print(f"ğŸ“ˆ æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {accuracy:.2f}%")
    print("ğŸ”§ ä½¿ç”¨çš„æŠ€æœ¯:")
    print("   âœ… åŠ æƒæŸå¤±å‡½æ•° (å¤„ç†æ•°æ®ä¸å¹³è¡¡)")
    print("   âœ… æ•°æ®å¢å¼º (æ—‹è½¬ã€å™ªå£°ã€ç¼©æ”¾)")
    print("   âœ… æ—©åœæœºåˆ¶ (é˜²æ­¢è¿‡æ‹Ÿåˆ)")
    print("   âœ… å­¦ä¹ ç‡è°ƒåº¦ (ä¼˜åŒ–æ”¶æ•›)")
    print("   âœ… PETerç½‘ç»œæ¶æ„ (EdgeConv + Transformer)")
    print("="*60)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ PETeræ¨¡å‹è®­ç»ƒå¼€å§‹")
    print("=" * 60)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®
    data_paths, labels, sample_indices = load_training_data()
    
    if len(data_paths) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ•°æ®")
        return
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader, test_loader, y_train = create_data_loaders(data_paths, labels, sample_indices)
    
    # è®¡ç®—ç±»åˆ«æƒé‡ä»¥å¤„ç†æ•°æ®ä¸å¹³è¡¡
    class_weights = calculate_class_weights(y_train)
    
    # åˆ›å»ºæ¨¡å‹
    model = PETerNetwork(num_classes=3, num_points=100, num_frames=25, k=10)
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # è®­ç»ƒæ¨¡å‹ï¼ˆä½¿ç”¨åŠ æƒæŸå¤±ï¼‰
    train_losses, val_losses, train_accs, val_accs = train_model(
        model, train_loader, val_loader, num_epochs=50, learning_rate=1e-4, device=device, class_weights=class_weights
    )
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(torch.load('best_peter_model.pth'))
    
    # è¯„ä¼°æ¨¡å‹
    predictions, targets, accuracy = evaluate_model(model, test_loader, device)
    
    # ç»˜åˆ¶ç»“æœ
    plot_training_history(train_losses, val_losses, train_accs, val_accs)
    plot_confusion_matrix(targets, predictions)
    
    # æ‰“å°è®­ç»ƒæ€»ç»“
    print_training_summary(class_weights, accuracy)
    
    print("âœ… è®­ç»ƒå®Œæˆ!")

if __name__ == "__main__":
    main() 