#!/usr/bin/env python3
"""
æ”¹è¿›ç‰ˆTIé›·è¾¾ç‚¹äº‘å¤šå¸§èåˆä¸èšç±»æ¸…æ´—å™¨
è§£å†³æ•°æ®ä¸¢å¤±é—®é¢˜ï¼Œæé«˜æ ·æœ¬æ•°é‡
"""

import json
import os
import numpy as np
from datetime import datetime
import glob
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

class ImprovedPointCloudFusionCleaner:
    def __init__(self, data_folder="radar_data"):
        """
        åˆå§‹åŒ–æ”¹è¿›ç‰ˆç‚¹äº‘èåˆæ¸…æ´—å™¨
        Args:
            data_folder: æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
        """
        self.data_folder = data_folder
        self.frames_per_sample = 6  # 6å¸§èåˆä¸ºä¸€ä¸ªæ ·æœ¬
        self.min_frames_per_sample = 4  # æœ€å°‘4å¸§å°±å¯ä»¥å½¢æˆæ ·æœ¬
        
    def load_pointcloud_data(self, file_path):
        """åŠ è½½ç‚¹äº‘æ•°æ®æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return data
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return []
    
    def extract_xyz_points(self, point_data):
        """
        ä»ç‚¹äº‘æ•°æ®ä¸­æå–x,y,zåæ ‡
        Args:
            point_data: ç‚¹äº‘æ•°æ®åˆ—è¡¨
        Returns:
            points: numpyæ•°ç»„ï¼Œå½¢çŠ¶ä¸º(n_points, 3)
        """
        points = []
        for item in point_data:
            if 'x' in item and 'y' in item and 'z' in item:
                points.append([item['x'], item['y'], item['z']])
        
        return np.array(points) if points else np.empty((0, 3))
    
    def group_frames_by_time(self, point_data, time_window=0.3):
        """
        æŒ‰æ—¶é—´çª—å£åˆ†ç»„å¸§æ•°æ®ï¼ˆæ”¹è¿›ï¼šå¢åŠ æ—¶é—´çª—å£ï¼‰
        Args:
            point_data: ç‚¹äº‘æ•°æ®åˆ—è¡¨
            time_window: æ—¶é—´çª—å£ï¼ˆç§’ï¼‰- ä»0.1å¢åŠ åˆ°0.3
        Returns:
            grouped_frames: åˆ†ç»„åçš„å¸§æ•°æ®
        """
        if not point_data:
            return []
        
        # æŒ‰æ—¶é—´æˆ³æ’åº
        sorted_data = sorted(point_data, key=lambda x: x.get('timestamp', 0))
        
        grouped_frames = []
        current_group = []
        last_timestamp = None
        
        for item in sorted_data:
            timestamp = item.get('timestamp', 0)
            
            if last_timestamp is None or (timestamp - last_timestamp) <= time_window:
                current_group.append(item)
            else:
                if current_group:
                    grouped_frames.append(current_group)
                current_group = [item]
            
            last_timestamp = timestamp
        
        # æ·»åŠ æœ€åä¸€ç»„
        if current_group:
            grouped_frames.append(current_group)
        
        return grouped_frames
    
    def multi_frame_fusion(self, frame_groups):
        """
        å¤šå¸§èåˆï¼šå°†å¤šå¸§çš„ç‚¹äº‘æ•°æ®èåˆ
        Args:
            frame_groups: åˆ†ç»„åçš„å¸§æ•°æ®
        Returns:
            fused_points: èåˆåçš„ç‚¹äº‘æ•°æ®
        """
        all_points = []
        
        for frame_group in frame_groups:
            points = self.extract_xyz_points(frame_group)
            if len(points) > 0:
                all_points.append(points)
        
        if not all_points:
            return np.empty((0, 3))
        
        # å°†æ‰€æœ‰å¸§çš„ç‚¹äº‘åˆå¹¶
        fused_points = np.vstack(all_points)
        return fused_points
    
    def dbscan_clustering(self, points, eps=1.0, min_samples=2):
        """
        DBSCANèšç±»ï¼Œåˆ†ç¦»ç›®æ ‡å’Œå™ªå£°ç‚¹ï¼ˆæ”¹è¿›ï¼šæ”¾å®½å‚æ•°ï¼‰
        Args:
            points: ç‚¹äº‘æ•°æ®ï¼Œå½¢çŠ¶ä¸º(n_points, 3)
            eps: é‚»åŸŸåŠå¾„ - ä»0.5å¢åŠ åˆ°1.0
            min_samples: æœ€å°æ ·æœ¬æ•° - ä»3å‡å°‘åˆ°2
        Returns:
            target_points: ç›®æ ‡ç‚¹äº‘
            noise_points: å™ªå£°ç‚¹äº‘
        """
        if len(points) == 0:
            return np.empty((0, 3)), np.empty((0, 3))
        
        # æ ‡å‡†åŒ–æ•°æ®
        scaler = StandardScaler()
        points_scaled = scaler.fit_transform(points)
        
        # DBSCANèšç±»
        clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(points_scaled)
        labels = clustering.labels_
        
        # åˆ†ç¦»ç›®æ ‡å’Œå™ªå£°ç‚¹
        target_mask = labels != -1  # éå™ªå£°ç‚¹
        noise_mask = labels == -1   # å™ªå£°ç‚¹
        
        target_points = points[target_mask]
        noise_points = points[noise_mask]
        
        return target_points, noise_points
    
    def calculate_density_features(self, points):
        """
        è®¡ç®—ç‚¹äº‘å¯†åº¦ç‰¹å¾
        Args:
            points: ç‚¹äº‘æ•°æ®
        Returns:
            features: å¯†åº¦ç‰¹å¾å­—å…¸
        """
        if len(points) == 0:
            return {
                'num_points': 0,
                'point_density': 0.0,
                'spatial_volume': 0.0,
                'concentration_ratio': 0.0
            }
        
        # åŸºæœ¬ç»Ÿè®¡
        num_points = len(points)
        
        # è®¡ç®—ç©ºé—´èŒƒå›´
        x_range = np.max(points[:, 0]) - np.min(points[:, 0])
        y_range = np.max(points[:, 1]) - np.min(points[:, 1])
        z_range = np.max(points[:, 2]) - np.min(points[:, 2])
        
        # ç©ºé—´ä½“ç§¯
        spatial_volume = x_range * y_range * z_range
        
        # ç‚¹äº‘å¯†åº¦ï¼ˆç‚¹æ•°/ä½“ç§¯ï¼‰
        point_density = num_points / (spatial_volume + 1e-6)
        
        # è®¡ç®—ç‚¹äº‘ä¸­å¿ƒ
        center = np.mean(points, axis=0)
        
        # è®¡ç®—åˆ°ä¸­å¿ƒçš„è·ç¦»
        distances = np.linalg.norm(points - center, axis=1)
        
        # é›†ä¸­åº¦æ¯”ç‡ï¼ˆè¿‘è·ç¦»ç‚¹çš„æ¯”ä¾‹ï¼‰
        concentration_ratio = np.sum(distances < np.mean(distances)) / num_points
        
        return {
            'num_points': num_points,
            'point_density': float(point_density),
            'spatial_volume': float(spatial_volume),
            'concentration_ratio': float(concentration_ratio),
            'x_range': float(x_range),
            'y_range': float(y_range),
            'z_range': float(z_range),
            'center_x': float(center[0]),
            'center_y': float(center[1]),
            'center_z': float(center[2])
        }
    
    def create_fused_sample(self, frame_groups):
        """
        åˆ›å»ºèåˆæ ·æœ¬ï¼ˆæ”¹è¿›ï¼šå…è®¸éƒ¨åˆ†å¸§æ•°ä¸è¶³ï¼‰
        Args:
            frame_groups: åˆ†ç»„åçš„å¸§æ•°æ®
        Returns:
            sample: èåˆåçš„æ ·æœ¬
        """
        if len(frame_groups) < self.min_frames_per_sample:
            return None
        
        # å–å‰6å¸§è¿›è¡Œèåˆï¼Œå¦‚æœä¸è¶³6å¸§å°±ç”¨æ‰€æœ‰å¯ç”¨å¸§
        sample_frames = frame_groups[:min(len(frame_groups), self.frames_per_sample)]
        
        # å¤šå¸§èåˆ
        fused_points = self.multi_frame_fusion(sample_frames)
        
        if len(fused_points) == 0:
            return None
        
        # DBSCANèšç±»
        target_points, noise_points = self.dbscan_clustering(fused_points)
        
        # è®¡ç®—ç‰¹å¾
        target_features = self.calculate_density_features(target_points)
        noise_features = self.calculate_density_features(noise_points)
        
        # åˆ›å»ºæ ·æœ¬
        sample = {
            'sample_id': 0,  # å°†åœ¨å¤–éƒ¨è®¾ç½®
            'start_timestamp': sample_frames[0][0].get('timestamp', 0),
            'end_timestamp': sample_frames[-1][-1].get('timestamp', 0),
            'num_frames': len(sample_frames),
            'target_points': target_points.tolist() if len(target_points) > 0 else [],
            'noise_points': noise_points.tolist() if len(noise_points) > 0 else [],
            'target_features': target_features,
            'noise_features': noise_features,
            'fusion_ratio': len(target_points) / (len(target_points) + len(noise_points) + 1e-6)
        }
        
        return sample
    
    def create_improved_samples(self, frame_groups):
        """
        åˆ›å»ºæ”¹è¿›çš„æ ·æœ¬ï¼ˆå…è®¸é‡å é‡‡æ ·ï¼‰
        Args:
            frame_groups: åˆ†ç»„åçš„å¸§æ•°æ®
        Returns:
            samples: èåˆåçš„æ ·æœ¬åˆ—è¡¨
        """
        samples = []
        
        # ä½¿ç”¨æ»‘åŠ¨çª—å£ï¼Œå…è®¸é‡å 
        for i in range(0, len(frame_groups) - self.min_frames_per_sample + 1):
            sample_frames = frame_groups[i:i + self.frames_per_sample]
            
            # å¦‚æœä¸è¶³6å¸§ï¼Œå°±ç”¨æ‰€æœ‰å¯ç”¨å¸§
            if len(sample_frames) >= self.min_frames_per_sample:
                sample = self.create_fused_sample(sample_frames)
                if sample:
                    sample['sample_id'] = len(samples)
                    samples.append(sample)
        
        return samples
    
    def clean_and_save_data(self, input_file, output_file, visualize=False):
        """
        æ¸…æ´—å¹¶ä¿å­˜æ•°æ®
        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            visualize: æ˜¯å¦ç”Ÿæˆå¯è§†åŒ–
        """
        print(f"ğŸ§¹ æ¸…æ´—æ•°æ®: {input_file}")
        
        # åŠ è½½åŸå§‹æ•°æ®
        raw_data = self.load_pointcloud_data(input_file)
        if not raw_data:
            print(f"âš ï¸  æ–‡ä»¶ä¸ºç©ºæˆ–åŠ è½½å¤±è´¥: {input_file}")
            return
        
        print(f"ğŸ“Š åŸå§‹æ•°æ®: {len(raw_data)} ä¸ªæ•°æ®ç‚¹")
        
        # æŒ‰æ—¶é—´åˆ†ç»„ï¼ˆä½¿ç”¨æ”¹è¿›çš„æ—¶é—´çª—å£ï¼‰
        frame_groups = self.group_frames_by_time(raw_data, time_window=0.3)
        print(f"ğŸ“¦ åˆ†ç»„å: {len(frame_groups)} ä¸ªæ—¶é—´çª—å£")
        
        # åˆ›å»ºæ”¹è¿›çš„èåˆæ ·æœ¬
        samples = self.create_improved_samples(frame_groups)
        print(f"ğŸ¯ åˆ›å»ºæ ·æœ¬: {len(samples)} ä¸ªèåˆæ ·æœ¬")
        
        # ä¿å­˜æ¸…æ´—åçš„æ•°æ®
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(samples, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ æ¸…æ´—åæ•°æ®å·²ä¿å­˜: {output_file}")
        
        return samples
    
    def process_all_data(self, visualize=False):
        """å¤„ç†æ‰€æœ‰æ•°æ®æ–‡ä»¶å¤¹"""
        print("ğŸš€ å¼€å§‹æ”¹è¿›ç‰ˆæ•°æ®èåˆæ¸…æ´—")
        print("=" * 60)
        
        # æŸ¥æ‰¾æ‰€æœ‰æ•°æ®æ–‡ä»¶å¤¹
        data_dirs = ['sit', 'squat', 'stand']
        
        for data_dir in data_dirs:
            dir_path = os.path.join(self.data_folder, data_dir)
            if not os.path.exists(dir_path):
                print(f"âš ï¸  æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {dir_path}")
                continue
            
            print(f"\nğŸ“ å¤„ç†æ–‡ä»¶å¤¹: {data_dir}")
            
            # åªæŸ¥æ‰¾pointcloudæ–‡ä»¶å¤¹ä¸­çš„JSONæ–‡ä»¶
            pointcloud_dir = os.path.join(dir_path, "pointcloud")
            if os.path.exists(pointcloud_dir):
                json_files = glob.glob(os.path.join(pointcloud_dir, "*.json"))
            else:
                json_files = []
            
            if not json_files:
                print(f"âš ï¸  æœªæ‰¾åˆ°JSONæ–‡ä»¶: {dir_path}")
                continue
            
            # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
            output_dir = os.path.join(self.data_folder, f"{data_dir}_improved_fused")
            os.makedirs(output_dir, exist_ok=True)
            
            total_samples = 0
            
            for json_file in json_files:
                # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
                rel_path = os.path.relpath(json_file, dir_path)
                output_file = os.path.join(output_dir, f"improved_fused_{rel_path}")
                
                # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(output_file), exist_ok=True)
                
                # æ¸…æ´—æ•°æ®
                samples = self.clean_and_save_data(json_file, output_file, visualize)
                if samples:
                    total_samples += len(samples)
            
            print(f"âœ… {data_dir} å¤„ç†å®Œæˆï¼Œå…±ç”Ÿæˆ {total_samples} ä¸ªæ”¹è¿›èåˆæ ·æœ¬")
        
        print("\nğŸ‰ æ‰€æœ‰æ•°æ®æ”¹è¿›èåˆæ¸…æ´—å®Œæˆï¼")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§¹ æ”¹è¿›ç‰ˆç‚¹äº‘èåˆæ¸…æ´—å™¨")
    print("=" * 60)
    
    # åˆ›å»ºæ”¹è¿›ç‰ˆæ¸…æ´—å™¨
    cleaner = ImprovedPointCloudFusionCleaner("radar_data")
    
    # å¤„ç†æ‰€æœ‰æ•°æ®
    cleaner.process_all_data(visualize=False)

if __name__ == "__main__":
    main() 